{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_accuracy' from 'utils' (C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_accuracy, plot_curve, keep_store_dict, store_dict_to_disk\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_accuracy' from 'utils' (C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from utils import get_accuracy, plot_curve, keep_store_dict, store_dict_to_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>r ankle_X</th>\n",
       "      <th>r ankle_Y</th>\n",
       "      <th>r knee_X</th>\n",
       "      <th>r knee_Y</th>\n",
       "      <th>r hip_X</th>\n",
       "      <th>r hip_Y</th>\n",
       "      <th>l hip_X</th>\n",
       "      <th>l hip_Y</th>\n",
       "      <th>...</th>\n",
       "      <th>r shoulder_Y</th>\n",
       "      <th>l shoulder_X</th>\n",
       "      <th>l shoulder_Y</th>\n",
       "      <th>l elbow_X</th>\n",
       "      <th>l elbow_Y</th>\n",
       "      <th>l wrist_X</th>\n",
       "      <th>l wrist_Y</th>\n",
       "      <th>Scale</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>060111501.jpg</td>\n",
       "      <td>980</td>\n",
       "      <td>322</td>\n",
       "      <td>896</td>\n",
       "      <td>318</td>\n",
       "      <td>865</td>\n",
       "      <td>248</td>\n",
       "      <td>943</td>\n",
       "      <td>226</td>\n",
       "      <td>...</td>\n",
       "      <td>147</td>\n",
       "      <td>923</td>\n",
       "      <td>123</td>\n",
       "      <td>995</td>\n",
       "      <td>163</td>\n",
       "      <td>961</td>\n",
       "      <td>223</td>\n",
       "      <td>3.806403</td>\n",
       "      <td>curling</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>002058449.jpg</td>\n",
       "      <td>918</td>\n",
       "      <td>456</td>\n",
       "      <td>659</td>\n",
       "      <td>518</td>\n",
       "      <td>713</td>\n",
       "      <td>413</td>\n",
       "      <td>979</td>\n",
       "      <td>288</td>\n",
       "      <td>...</td>\n",
       "      <td>270</td>\n",
       "      <td>877</td>\n",
       "      <td>247</td>\n",
       "      <td>1112</td>\n",
       "      <td>384</td>\n",
       "      <td>1012</td>\n",
       "      <td>489</td>\n",
       "      <td>8.280870</td>\n",
       "      <td>curling</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>029122914.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>301</td>\n",
       "      <td>473</td>\n",
       "      <td>302</td>\n",
       "      <td>346</td>\n",
       "      <td>362</td>\n",
       "      <td>345</td>\n",
       "      <td>...</td>\n",
       "      <td>220</td>\n",
       "      <td>371</td>\n",
       "      <td>213</td>\n",
       "      <td>396</td>\n",
       "      <td>309</td>\n",
       "      <td>393</td>\n",
       "      <td>290</td>\n",
       "      <td>2.204083</td>\n",
       "      <td>truck driving, loading and unloading truck, ty...</td>\n",
       "      <td>occupation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>061185289.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>288</td>\n",
       "      <td>552</td>\n",
       "      <td>355</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.143112</td>\n",
       "      <td>truck driving, loading and unloading truck, ty...</td>\n",
       "      <td>occupation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>013949386.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>110</td>\n",
       "      <td>385</td>\n",
       "      <td>208</td>\n",
       "      <td>355</td>\n",
       "      <td>...</td>\n",
       "      <td>303</td>\n",
       "      <td>212</td>\n",
       "      <td>153</td>\n",
       "      <td>319</td>\n",
       "      <td>123</td>\n",
       "      <td>376</td>\n",
       "      <td>39</td>\n",
       "      <td>4.431105</td>\n",
       "      <td>truck driving, loading and unloading truck, ty...</td>\n",
       "      <td>occupation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11426</th>\n",
       "      <td>17367</td>\n",
       "      <td>050240222.jpg</td>\n",
       "      <td>607</td>\n",
       "      <td>689</td>\n",
       "      <td>599</td>\n",
       "      <td>542</td>\n",
       "      <td>605</td>\n",
       "      <td>416</td>\n",
       "      <td>691</td>\n",
       "      <td>410</td>\n",
       "      <td>...</td>\n",
       "      <td>232</td>\n",
       "      <td>663</td>\n",
       "      <td>228</td>\n",
       "      <td>623</td>\n",
       "      <td>289</td>\n",
       "      <td>555</td>\n",
       "      <td>333</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11427</th>\n",
       "      <td>17368</td>\n",
       "      <td>033474347.jpg</td>\n",
       "      <td>876</td>\n",
       "      <td>461</td>\n",
       "      <td>815</td>\n",
       "      <td>380</td>\n",
       "      <td>779</td>\n",
       "      <td>296</td>\n",
       "      <td>742</td>\n",
       "      <td>303</td>\n",
       "      <td>...</td>\n",
       "      <td>214</td>\n",
       "      <td>663</td>\n",
       "      <td>223</td>\n",
       "      <td>674</td>\n",
       "      <td>286</td>\n",
       "      <td>631</td>\n",
       "      <td>260</td>\n",
       "      <td>2.003677</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11428</th>\n",
       "      <td>17369</td>\n",
       "      <td>082650067.jpg</td>\n",
       "      <td>1038</td>\n",
       "      <td>656</td>\n",
       "      <td>950</td>\n",
       "      <td>504</td>\n",
       "      <td>903</td>\n",
       "      <td>328</td>\n",
       "      <td>831</td>\n",
       "      <td>337</td>\n",
       "      <td>...</td>\n",
       "      <td>204</td>\n",
       "      <td>729</td>\n",
       "      <td>220</td>\n",
       "      <td>704</td>\n",
       "      <td>332</td>\n",
       "      <td>675</td>\n",
       "      <td>269</td>\n",
       "      <td>3.235820</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11429</th>\n",
       "      <td>17370</td>\n",
       "      <td>072772110.jpg</td>\n",
       "      <td>562</td>\n",
       "      <td>617</td>\n",
       "      <td>558</td>\n",
       "      <td>539</td>\n",
       "      <td>553</td>\n",
       "      <td>452</td>\n",
       "      <td>511</td>\n",
       "      <td>458</td>\n",
       "      <td>...</td>\n",
       "      <td>377</td>\n",
       "      <td>490</td>\n",
       "      <td>378</td>\n",
       "      <td>470</td>\n",
       "      <td>411</td>\n",
       "      <td>461</td>\n",
       "      <td>414</td>\n",
       "      <td>1.494952</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11430</th>\n",
       "      <td>17371</td>\n",
       "      <td>039361034.jpg</td>\n",
       "      <td>496</td>\n",
       "      <td>377</td>\n",
       "      <td>466</td>\n",
       "      <td>322</td>\n",
       "      <td>510</td>\n",
       "      <td>278</td>\n",
       "      <td>540</td>\n",
       "      <td>280</td>\n",
       "      <td>...</td>\n",
       "      <td>253</td>\n",
       "      <td>466</td>\n",
       "      <td>251</td>\n",
       "      <td>436</td>\n",
       "      <td>293</td>\n",
       "      <td>410</td>\n",
       "      <td>325</td>\n",
       "      <td>1.459468</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11431 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID           NAME  r ankle_X  r ankle_Y  r knee_X  r knee_Y  \\\n",
       "0          5  060111501.jpg        980        322       896       318   \n",
       "1          7  002058449.jpg        918        456       659       518   \n",
       "2         14  029122914.jpg         -1         -1       301       473   \n",
       "3         15  061185289.jpg         -1         -1        -1        -1   \n",
       "4         16  013949386.jpg         -1         -1        -1        -1   \n",
       "...      ...            ...        ...        ...       ...       ...   \n",
       "11426  17367  050240222.jpg        607        689       599       542   \n",
       "11427  17368  033474347.jpg        876        461       815       380   \n",
       "11428  17369  082650067.jpg       1038        656       950       504   \n",
       "11429  17370  072772110.jpg        562        617       558       539   \n",
       "11430  17371  039361034.jpg        496        377       466       322   \n",
       "\n",
       "       r hip_X  r hip_Y  l hip_X  l hip_Y  ...  r shoulder_Y  l shoulder_X  \\\n",
       "0          865      248      943      226  ...           147           923   \n",
       "1          713      413      979      288  ...           270           877   \n",
       "2          302      346      362      345  ...           220           371   \n",
       "3           -1       -1       -1       -1  ...           288           552   \n",
       "4          110      385      208      355  ...           303           212   \n",
       "...        ...      ...      ...      ...  ...           ...           ...   \n",
       "11426      605      416      691      410  ...           232           663   \n",
       "11427      779      296      742      303  ...           214           663   \n",
       "11428      903      328      831      337  ...           204           729   \n",
       "11429      553      452      511      458  ...           377           490   \n",
       "11430      510      278      540      280  ...           253           466   \n",
       "\n",
       "       l shoulder_Y  l elbow_X  l elbow_Y  l wrist_X  l wrist_Y     Scale  \\\n",
       "0               123        995        163        961        223  3.806403   \n",
       "1               247       1112        384       1012        489  8.280870   \n",
       "2               213        396        309        393        290  2.204083   \n",
       "3               355         -1         -1         -1         -1  4.143112   \n",
       "4               153        319        123        376         39  4.431105   \n",
       "...             ...        ...        ...        ...        ...       ...   \n",
       "11426           228        623        289        555        333  3.480000   \n",
       "11427           223        674        286        631        260  2.003677   \n",
       "11428           220        704        332        675        269  3.235820   \n",
       "11429           378        470        411        461        414  1.494952   \n",
       "11430           251        436        293        410        325  1.459468   \n",
       "\n",
       "                                                Activity        Category  \n",
       "0                                                curling          sports  \n",
       "1                                                curling          sports  \n",
       "2      truck driving, loading and unloading truck, ty...      occupation  \n",
       "3      truck driving, loading and unloading truck, ty...      occupation  \n",
       "4      truck driving, loading and unloading truck, ty...      occupation  \n",
       "...                                                  ...             ...  \n",
       "11426                                        pushing car  transportation  \n",
       "11427                                        pushing car  transportation  \n",
       "11428                                        pushing car  transportation  \n",
       "11429                                        pushing car  transportation  \n",
       "11430                                        pushing car  transportation  \n",
       "\n",
       "[11431 rows x 37 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename='mpii_new.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, heat_maps_dir, img_dir, transform, target_transform=None):\n",
    "        self.heat_maps_dir = heat_maps_dir\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform = transforms.Compose([\n",
    "            transforms.Resize(transform),\n",
    "            ])\n",
    "        self.target_transform = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, os.listdir(self.img_dir)[idx])\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        map_path = os.path.join(self.heat_maps_dir, os.listdir(self.heat_maps_dir)[idx])\n",
    "        mapa = np.load(map_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            mapa = self.target_transform(mapa)\n",
    "        return image, mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = ImageDataset(\"heat_maps_test_plain\", \"images_test\", (368, 368))\n",
    "dataset_training = ImageDataset(\"heat_maps_training_plain\", \"images_training\", (368, 368))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" data_augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),  # 0.1 radians is approximately 10 degrees\n",
    "    transforms.RandomResizedCrop(368, scale=(0.9, 1.0), ratio=(1.0, 1.0))  # Assuming 10% zoom translates to a scale of (0.9, 1.0)\n",
    "]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_initializer(mean=0.0, stddev=0.01):\n",
    "    def _initializer(tensor):\n",
    "        return nn.init.normal_(tensor, mean=mean, std=stddev)\n",
    "    return _initializer\n",
    "\n",
    "def constant_initializer(value=0.0):\n",
    "    def _initializer(tensor):\n",
    "        return nn.init.constant_(tensor, value)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAVE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelAVE, self).__init__()\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=9, stride=8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        center_map = x[:, 3:, :, :]\n",
    "        return self.avg_pool(center_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model1, self).__init__()\n",
    "        self.conv1_stage1 = nn.Conv2d(3, 128, kernel_size=9, padding=4)\n",
    "        self.conv2_stage1 = nn.Conv2d(128, 128, kernel_size=9, padding=4)\n",
    "        self.conv3_stage1 = nn.Conv2d(128, 128, kernel_size=9, padding=4)\n",
    "        self.conv4_stage1 = nn.Conv2d(128, 32, kernel_size=5, padding=2)\n",
    "        self.conv5_stage1 = nn.Conv2d(32, 512, kernel_size=9, padding=4)\n",
    "        self.conv6_stage1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.conv7_stage1 = nn.Conv2d(512, 15, kernel_size=1)\n",
    "        self.pool_center_lower = None\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    gaussian_initializer()(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        constant_initializer()(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        image = x[:, :3, :, :]\n",
    "        x1 = F.relu(self.conv1_stage1(image))\n",
    "        x1 = F.max_pool2d(x1, kernel_size=3, stride=2)\n",
    "        x1 = F.relu(self.conv2_stage1(x1))\n",
    "        x1 = F.max_pool2d(x1, kernel_size=3, stride=2)\n",
    "        x1 = F.relu(self.conv3_stage1(x1))\n",
    "        x1 = F.max_pool2d(x1, kernel_size=3, stride=2)\n",
    "        x1 = F.relu(self.conv4_stage1(x1))\n",
    "        x1 = F.relu(self.conv5_stage1(x1))\n",
    "        x1 = F.relu(self.conv6_stage1(x1))\n",
    "        x1 = self.conv7_stage1(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "        self.conv1_stage2 = nn.Conv2d(3, 128, kernel_size=9, padding=4)\n",
    "        self.conv2_stage2 = nn.Conv2d(128, 128, kernel_size=9, padding=4)\n",
    "        self.conv3_stage2 = nn.Conv2d(128, 128, kernel_size=9, padding=4)\n",
    "        self.conv4_stage2 = nn.Conv2d(128, 32, kernel_size=5, padding=2)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    gaussian_initializer()(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        constant_initializer()(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        image = x[:, :3, :, :]\n",
    "        x2 = F.relu(self.conv1_stage2(image))\n",
    "        x2 = F.max_pool2d(x2, kernel_size=3, stride=2)\n",
    "        x2 = F.relu(self.conv2_stage2(x2))\n",
    "        x2 = F.max_pool2d(x2, kernel_size=3, stride=2)\n",
    "        x2 = F.relu(self.conv3_stage2(x2))\n",
    "        x3 = F.max_pool2d(x2, kernel_size=3, stride=2)\n",
    "        x2 = F.relu(self.conv4_stage2(x3))\n",
    "        return x2, x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concatenated(nn.Module):\n",
    "    def __init__(self, model1, model2, modelAVE):\n",
    "        super(Concatenated, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.modelAVE = modelAVE\n",
    "    \n",
    "    def forward(self, x):\n",
    "        image = x[:, :3, :, :]\n",
    "        center_map = x[:, 3:, :, :]\n",
    "        x1 = self.model1(image)\n",
    "        x2 = self.model2(image)\n",
    "        x_ave = self.modelAVE(center_map)\n",
    "        concatenated_output = torch.cat([x1, x2, x_ave], dim=1)\n",
    "        return concatenated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelM, self).__init__()\n",
    "        self.Mconv1 = nn.Conv2d(79, 128, kernel_size=11, padding=5)\n",
    "        self.Mconv2 = nn.Conv2d(128, 128, kernel_size=11, padding=5)\n",
    "        self.Mconv3 = nn.Conv2d(128, 128, kernel_size=11, padding=5)\n",
    "        self.Mconv4 = nn.Conv2d(128, 128, kernel_size=1)\n",
    "        self.Mconv5 = nn.Conv2d(128, 15, kernel_size=1)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    gaussian_initializer()(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        constant_initializer()(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        image = x[:, :3, :, :]\n",
    "        x2 = F.relu(self.Mconv1_stage2(image))\n",
    "        x2 = F.relu(self.Mconv2_stage2(x2))\n",
    "        x2 = F.relu(self.Mconv3_stage2(x2))\n",
    "        x2 = F.relu(self.Mconv4_stage2(x2))\n",
    "        x2 = F.relu(self.Mconv5_stage2(x2))\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model3, self).__init__()\n",
    "        self.conv1_stage3 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n",
    "        \n",
    "\n",
    "    def _initialize_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    gaussian_initializer()(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        constant_initializer()(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        image = x[:, :3, :, :]\n",
    "        x3 = F.relu(self.conv1_stage3(image))\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model1()\n",
    "model2 = Model2()\n",
    "modelAVE = ModelAVE()\n",
    "concatenated_stage2 = Concatenated(model1, model2, modelAVE)\n",
    "model2_M = ModelM()\n",
    "model3 = Model3()\n",
    "concatenated_stage3 = Concatenated(model3, model2_M, modelAVE)\n",
    "model3_M = ModelM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model1, model2, model3, modelAVE, concatenated_stage2, concatenated_stage3, model2_M, model3_M):\n",
    "        super(Model, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3\n",
    "        self.modelAVE = modelAVE\n",
    "        self.concatenated2 = concatenated_stage2\n",
    "        self.concatenated3 = concatenated_stage3\n",
    "        self.model2_M = model2_M\n",
    "        self.model3_M = model3_M \n",
    "        \n",
    "\n",
    "    def _initialize_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    gaussian_initializer()(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        constant_initializer()(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output1 = self.model1(x)\n",
    "        output2, input5 = self.model2(x)\n",
    "        outputAVE = self.modelAVE(x)\n",
    "        concatenated_output_stage2 = self.concatenated2(output1, output2, outputAVE)\n",
    "        output3 = self.model2_M(concatenated_output_stage2)     \n",
    "        output4 = self.model3(input5)\n",
    "        concatenated_output_stage3 = self.concatenated3(output3, outputAVE, output4)\n",
    "        output5 = self.model3_M\n",
    "        concatenated_output = torch.cat([output1, output2, output3,concatenated_output_stage2, outputAVE, concatenated_output_stage2, concatenated_output_stage3, output4, output5], dim=1)\n",
    "        return concatenated_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model1): Model1(\n",
       "    (conv1_stage1): Conv2d(3, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (conv2_stage1): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (conv3_stage1): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (conv4_stage1): Conv2d(128, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv5_stage1): Conv2d(32, 512, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (conv6_stage1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv7_stage1): Conv2d(512, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (model2): Model2(\n",
       "    (conv1_stage2): Conv2d(3, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (conv2_stage2): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (conv3_stage2): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (conv4_stage2): Conv2d(128, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       "  (model3): Model3(\n",
       "    (conv1_stage3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       "  (modelAVE): ModelAVE(\n",
       "    (avg_pool): AvgPool2d(kernel_size=9, stride=8, padding=0)\n",
       "  )\n",
       "  (concatenated2): Concatenated(\n",
       "    (model1): Model1(\n",
       "      (conv1_stage1): Conv2d(3, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "      (conv2_stage1): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "      (conv3_stage1): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "      (conv4_stage1): Conv2d(128, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (conv5_stage1): Conv2d(32, 512, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "      (conv6_stage1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv7_stage1): Conv2d(512, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (model2): Model2(\n",
       "      (conv1_stage2): Conv2d(3, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "      (conv2_stage2): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "      (conv3_stage2): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "      (conv4_stage2): Conv2d(128, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (modelAVE): ModelAVE(\n",
       "      (avg_pool): AvgPool2d(kernel_size=9, stride=8, padding=0)\n",
       "    )\n",
       "  )\n",
       "  (concatenated3): Concatenated(\n",
       "    (model1): Model3(\n",
       "      (conv1_stage3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (model2): ModelM(\n",
       "      (Mconv1): Conv2d(79, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "      (Mconv2): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "      (Mconv3): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "      (Mconv4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (Mconv5): Conv2d(128, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (modelAVE): ModelAVE(\n",
       "      (avg_pool): AvgPool2d(kernel_size=9, stride=8, padding=0)\n",
       "    )\n",
       "  )\n",
       "  (model2_M): ModelM(\n",
       "    (Mconv1): Conv2d(79, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "    (Mconv2): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "    (Mconv3): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "    (Mconv4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (Mconv5): Conv2d(128, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (model3_M): ModelM(\n",
       "    (Mconv1): Conv2d(79, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "    (Mconv2): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "    (Mconv3): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "    (Mconv4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (Mconv5): Conv2d(128, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(model1, model2, model3, modelAVE, concatenated_stage2, concatenated_stage3, model2_M, model3_M)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    # Put the model in evaluation mode. \n",
    "    # Tells the model not to compute gradients. Increases inference speed.\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (x, y) in enumerate(test_loader, 0):\n",
    "            # Put the data to the appropriate device.\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # Do inference. Forwad pass with the model.\n",
    "            y_hat = model(x)\n",
    "            test_acc += get_accuracy(y_hat, y)\n",
    "    return test_acc / batch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, train_loader, store_dict, test_loader=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_running_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        model = model.train()\n",
    "        \n",
    "        for batch_num, (x, y) in enumerate(train_loader):\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = model(x)\n",
    "            loss = loss_function(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            # Do backpropagation algorithm and calculate all relevant gradients.\n",
    "            loss.backward()\n",
    "            # Update model parameters (weights and biases) w.r.t. computed gradients.\n",
    "            optimizer.step()\n",
    "\n",
    "            train_running_loss += loss.detach().item()\n",
    "            train_acc += get_accuracy(y_hat=y_hat, y=y)\n",
    "\n",
    "        \n",
    "        epoch_loss = train_running_loss / batch_num\n",
    "        epoch_acc = train_acc / batch_num\n",
    "        \n",
    "        store_dict = keep_store_dict(curve=epoch_loss, label='train_loss', store_dict=store_dict)\n",
    "        store_dict = keep_store_dict(curve=epoch_acc, label='train_acc', store_dict=store_dict)\n",
    "        print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n",
    "            %(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "        if test_loader is not None:\n",
    "            test_acc = test(model=model, test_loader=test_loader)\n",
    "            store_dict = keep_store_dict(curve=test_acc, label='test_acc', store_dict=store_dict)\n",
    "        \n",
    "    return store_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dict = train(model=model, num_epochs=5, train_loader=dataset_training, test_loader=dataset_test, store_dict=None)\n",
    "store_dict_to_disk(store_dict=store_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
